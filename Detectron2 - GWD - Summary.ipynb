{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Detectron2 - GWD - Summary.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cc3d408fc88a4da394be973409a66f8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d5dbc6e450e14beca384c67412b809a7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_239d27b8419049c88a80c88e8827c2a2","IPY_MODEL_94d77e61080046b2b3c5ee9328d5009b"]}},"d5dbc6e450e14beca384c67412b809a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"239d27b8419049c88a80c88e8827c2a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fc79a3f9784f4b39978e16c79284bded","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":3373,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3373,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8d702ca844d494190bd4e91be54e871"}},"94d77e61080046b2b3c5ee9328d5009b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ed8df33d9bb6404aa27d763850942f88","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3373/3373 [00:01&lt;00:00, 2853.94it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0af26a4f320b469ca17796518d2d4490"}},"fc79a3f9784f4b39978e16c79284bded":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c8d702ca844d494190bd4e91be54e871":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ed8df33d9bb6404aa27d763850942f88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0af26a4f320b469ca17796518d2d4490":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"fcua3rk_eEPd","colab_type":"text"},"source":["The NVIDIA System Management Interface (nvidia-smi) is a command line utility, based on top of the NVIDIA Management Library (NVML), intended to aid in the management and monitoring of NVIDIA GPU devices. "]},{"cell_type":"code","metadata":{"id":"bqc6dZbPJuLG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"status":"ok","timestamp":1596726069877,"user_tz":-180,"elapsed":5882,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"616c0f73-dda3-4319-ad2e-628e3d88a608"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Aug  6 15:01:07 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A_e-GAcVefxf","colab_type":"text"},"source":["# install dependencies: "]},{"cell_type":"code","metadata":{"id":"SEWrVyuIed7K","colab_type":"code","colab":{}},"source":["# install dependencies: \n","# !pip install cython pyyaml==5.1\n","# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","# import torch, torchvision\n","# print(torch.__version__, torch.cuda.is_available())\n","# !gcc --version\n","\n","\n","!pip install pyyaml==5.1 pycocotools>=2.0.1\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version\n","# opencv is pre-installed on colab\n","assert torch.__version__.startswith(\"1.6\")\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QpJw66IdecGo","colab_type":"text"},"source":["# Import section "]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"aJ5aI4FFJa3G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1596726124571,"user_tz":-180,"elapsed":60457,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"dd962264-9cc8-49fd-c40d-d927b83e155a"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook\n","import random\n","import cv2\n","\n","\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","\n","\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","\n","from detectron2.engine import DefaultTrainer\n","from detectron2.data import DatasetCatalog, MetadataCatalog\n","\n","import os\n","import json\n","from detectron2.structures import BoxMode\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VWatrTgygABy","colab_type":"text"},"source":["#  Path Define + Load csv file "]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"S0HF2LawJa3L","colab_type":"code","colab":{}},"source":["train_df = pd.read_csv('/content/gdrive/My Drive/Global Wheat Detection/train.csv')\n","train_path = \"/content/gdrive/My Drive/Global Wheat Detection/train\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I060BRLegTsE","colab_type":"text"},"source":["## Show data frame : "]},{"cell_type":"code","metadata":{"trusted":true,"id":"35dn5u7RJa3P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1596726126032,"user_tz":-180,"elapsed":61861,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"b74d6219-d5ba-445b-8d05-b31a4bdb13ed"},"source":["train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>bbox</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>b6ab77fd7</td>\n","      <td>1024</td>\n","      <td>1024</td>\n","      <td>[834.0, 222.0, 56.0, 36.0]</td>\n","      <td>usask_1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>b6ab77fd7</td>\n","      <td>1024</td>\n","      <td>1024</td>\n","      <td>[226.0, 548.0, 130.0, 58.0]</td>\n","      <td>usask_1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>b6ab77fd7</td>\n","      <td>1024</td>\n","      <td>1024</td>\n","      <td>[377.0, 504.0, 74.0, 160.0]</td>\n","      <td>usask_1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>b6ab77fd7</td>\n","      <td>1024</td>\n","      <td>1024</td>\n","      <td>[834.0, 95.0, 109.0, 107.0]</td>\n","      <td>usask_1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>b6ab77fd7</td>\n","      <td>1024</td>\n","      <td>1024</td>\n","      <td>[26.0, 144.0, 124.0, 117.0]</td>\n","      <td>usask_1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    image_id  width  height                         bbox   source\n","0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n","1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n","2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n","3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n","4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"trusted":true,"id":"2X7HvuxJJa3U","colab_type":"code","colab":{}},"source":["# train_df['width'].unique()\n","# train_df['height'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QFZkC3VRgmLT","colab_type":"text"},"source":["## Show boxes example for one random image \n","\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"L16xC6l5Ja3Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1596726126038,"user_tz":-180,"elapsed":61812,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"ca245f5b-799f-4202-cea0-8a64418d3e52"},"source":["for g in train_df.groupby('image_id'):\n","    b = g[1]['bbox'].values\n","    print(type(b),b)\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'> ['[0, 654, 37, 111]' '[0, 817, 135, 98]' '[0, 192, 22, 81]'\n"," '[4, 342, 63, 38]' '[82, 334, 82, 81]' '[30, 296, 48, 49]'\n"," '[176, 316, 70, 54]' '[176, 126, 69, 51]' '[203, 38, 42, 85]'\n"," '[3, 142, 89, 58]' '[236, 0, 60, 25]' '[329, 0, 75, 57]'\n"," '[796, 0, 69, 96]' '[659, 24, 59, 90]' '[540, 81, 140, 80]'\n"," '[233, 152, 89, 51]' '[422, 159, 58, 50]' '[462, 153, 205, 64]'\n"," '[468, 210, 108, 53]' '[417, 235, 136, 88]' '[287, 257, 56, 51]'\n"," '[283, 322, 117, 76]' '[393, 329, 174, 100]' '[606, 346, 47, 57]'\n"," '[611, 286, 70, 56]' '[718, 305, 54, 69]' '[709, 179, 102, 80]'\n"," '[813, 191, 120, 65]' '[862, 121, 65, 52]' '[876, 400, 80, 104]'\n"," '[951, 422, 52, 55]' '[763, 414, 69, 54]' '[633, 462, 77, 45]'\n"," '[438, 436, 104, 51]' '[356, 448, 65, 50]' '[292, 418, 69, 79]'\n"," '[251, 528, 75, 62]' '[421, 501, 52, 49]' '[692, 487, 77, 66]'\n"," '[769, 474, 90, 101]' '[692, 685, 83, 58]' '[611, 710, 72, 92]'\n"," '[417, 635, 70, 71]' '[706, 768, 103, 57]' '[820, 755, 127, 56]'\n"," '[899, 730, 83, 52]' '[855, 850, 154, 154]' '[792, 939, 63, 85]'\n"," '[605, 875, 175, 90]' '[364, 832, 58, 64]' '[246, 929, 139, 77]'\n"," '[400, 937, 82, 87]' '[471, 899, 63, 60]' '[701, 506, 162, 87]'\n"," '[552, 404, 87, 74]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2X0TySuNhkx9","colab_type":"text"},"source":["#Data Preprocessing\n","Grouping all the boxes according to the name of the image"]},{"cell_type":"code","metadata":{"trusted":true,"id":"1793McVdJa3b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119,"referenced_widgets":["cc3d408fc88a4da394be973409a66f8c","d5dbc6e450e14beca384c67412b809a7","239d27b8419049c88a80c88e8827c2a2","94d77e61080046b2b3c5ee9328d5009b","fc79a3f9784f4b39978e16c79284bded","c8d702ca844d494190bd4e91be54e871","ed8df33d9bb6404aa27d763850942f88","0af26a4f320b469ca17796518d2d4490"]},"executionInfo":{"status":"ok","timestamp":1596726127086,"user_tz":-180,"elapsed":62830,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"4c64ad6a-48cb-4e00-d948-42ce895c9cf6"},"source":["total_data = []\n","for g in tqdm_notebook(train_df.groupby('image_id')):\n","    data = {}\n","    data['filename'] = g[0]\n","    data['bbox'] = g[1]['bbox'].values\n","    total_data.append(data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  \n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc3d408fc88a4da394be973409a66f8c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=3373.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BtYfCQszh2mb","colab_type":"text"},"source":["In total there are 3373 images"]},{"cell_type":"code","metadata":{"trusted":true,"id":"c9vcGnGeJa3e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596726127093,"user_tz":-180,"elapsed":62809,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"47d3a355-a70e-40d7-bca1-27a4acab1809"},"source":["len(total_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3373"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"KZ3jhRQ1iIPh","colab_type":"text"},"source":["# Arrange all the images in dictionary - Create Dataset\n","\n","\n","---\n","\n","\n","For standard tasks, we load the original dataset into list[dict] with a specification similar to COCO’s json annotations. This is our standard representation for a dataset.\n","\n","`file_name`: the full path to the image file. Rotation or flipping may be applied if the image has EXIF metadata.\n","\n","`height, width`: integer. The shape of the image.\n","\n","`image_id `(str or int): a unique id that identifies this image. Required by evaluation to identify the images, but a dataset may use it for different purposes.\n","\n","`bbox (list[float])`: list of 4 numbers representing the bounding box of the instance.\n","\n","`bbox_mode` (int): the format of bbox. It must be a member of structures.BoxMode. Currently supports: BoxMode.XYXY_ABS, BoxMode.XYWH_ABS.\n","\n","`category_id` (int): an integer in the range [0, num_categories-1] representing the category label. The value num_categories is reserved to represent the “background” category, if applicable.\n","\n","`annotations (list[dict])`: each dict corresponds to annotations of one instance in this image. Required by instance detection/segmentation or keypoint detection tasks, but can be an empty list. \n","\n","[standard-dataset-dicts](https://detectron2.readthedocs.io/tutorials/datasets.html#standard-dataset-dicts)"]},{"cell_type":"code","metadata":{"trusted":true,"id":"Zqz0tx3lJa3t","colab_type":"code","colab":{}},"source":["\n","def get_wheat_dicts(total_data):\n","    \n","    dataset_dicts = []\n","    for idx, v in enumerate(total_data):\n","        record = {}\n","        \n","        filename = os.path.join(train_path, v[\"filename\"]+'.jpg')\n","        height, width = 1024,1024\n","        \n","        record[\"file_name\"] = filename\n","        record[\"image_id\"] = idx\n","        record[\"height\"] = height\n","        record[\"width\"] = width\n","      \n","        \n","        objs = []\n","        for b in v['bbox']:\n","            b = json.loads(b)\n","            obj = {\n","                'bbox': list(b),\n","                'bbox_mode': BoxMode.XYWH_ABS,\n","                'category_id':0,\n","            }\n","            objs.append(obj)\n","            \n","        record[\"annotations\"] = objs\n","        dataset_dicts.append(record)\n","    return dataset_dicts\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zTWBJqO0RTm7","colab_type":"text"},"source":["#  Register a Dataset and “Metadata” for Datasets for Train and Val datasets\n","\n","***Register a Dataset - DatasetCatalog***\n","\n","The function can do arbitrary things and should returns the data in either of the following format:\n","\n","Detectron2’s standard dataset dict, This will make it work with many other builtin features in detectron2, so it’s recommended to use it when it’s sufficient.\n","\n","\n","---\n","\n","\n","***Metadata - MetadataCatalog***\n","\n","Metadata is a key-value mapping that contains information that’s shared among the entire dataset, and usually is used to interpret what’s in the dataset, e.g., names of classes, colors of classes, root of files, etc.\n","\n","If you register a new dataset through DatasetCatalog.register, you may also want to add its corresponding metadata through MetadataCatalog.get(dataset_name).some_key = some_value, to enable any features that need the metadata."]},{"cell_type":"code","metadata":{"trusted":true,"id":"9Bl1RL97Ja3w","colab_type":"code","colab":{}},"source":["\n","#split data 90% train 10% to val \n","index = int(0.9 * len(total_data))\n","train_data = total_data[:index]\n","val_data = total_data[index:]\n","\n","folders = ['train', 'val']\n","for i, d in enumerate([train_data,val_data]):\n","    DatasetCatalog.register(\"wheat_\" + folders[i], lambda d=d: get_wheat_dicts(d))\n","    MetadataCatalog.get(\"wheat_\" + folders[i]).set(thing_classes=[\"wheat\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKt2MS2CSpBn","colab_type":"text"},"source":["# Displays 3 random images from the train by using Visualizer and MetaData"]},{"cell_type":"markdown","metadata":{"id":"Wlfjq7ajRbrZ","colab_type":"text"},"source":["## Visualizer\n","\n","Visualizer that draws data about detection/segmentation on images.\n","\n","It contains methods like draw_{text,box,circle,line,binary_mask,polygon} that draw primitive objects to images\n","\n","\n","`visualizer.draw_dataset_dict`  - Draw annotations/segmentaions in Detectron2 Dataset format.\n","\n","Args:\n","\n","    dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.  \n","Returns:\n","\n","    output (VisImage): image object with visualizations. "]},{"cell_type":"code","metadata":{"trusted":true,"id":"28-pSHv4Ja30","colab_type":"code","colab":{}},"source":["wheat_metadata = MetadataCatalog.get(\"wheat_train\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"n0lR12aJJa33","colab_type":"code","colab":{}},"source":["# train_data[0]['bbox'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"PpHaxEpUJa35","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"10KN97RPIQTLJwpZKqXYMV6KL8JPzCJdR"},"executionInfo":{"status":"ok","timestamp":1596726171785,"user_tz":-180,"elapsed":9363,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"4f6a7b0c-c723-4f28-fdfb-8999a0115d86"},"source":["dataset_dicts = get_wheat_dicts(train_data)\n","for d in random.sample(dataset_dicts, 3):\n","    img = cv2.imread(d[\"file_name\"])\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=wheat_metadata, scale=1)\n","    vis = visualizer.draw_dataset_dict(d)\n","    plt.figure(figsize=[10, 20])\n","    plt.imshow(vis.get_image()[:,:,::-1])\n","    plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"ZYNk5HZHTmRD","colab_type":"text"},"source":["# RetinaNet \n","\n","\n","\n","---\n","\n","RetinaNet adopts the Feature Pyramid Network (FPN) proposed by Lin, Dollar, et al. (2017) as its backbone, which is in turn built on top of ResNet in a fully convolutional fashion. The fully convolutional nature enables the network to take an image of an arbitrary size and outputs proportionally sized feature maps at multiple levels in the feature pyramid.\n","\n","\n","![alt text](https://blog.zenggyu.com/post/2018-12-05/fig_1.jpg)\n","\n","[RetinaNet Explained and Demystified](https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/)\n","\n","\n","\n","\n","---\n","\n","## RetinaNet Setup\n","\n","cfg.merge_from_file - load values from a file yaml\n","cfg.MODEL.WEIGHTS  - load WEIGHTS from a file yaml\n","\n","```\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n","\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")  \n","```\n","\n","\n","\n","This is the number of foreground classes, we have only wheat class.\n","\n","```\n","  cfg.MODEL.RETINANET.NUM_CLASSES = 1\n","```\n","\n","\n","Inference cls score threshold, only anchors with score > INFERENCE_TH are considered for inference (to improve speed)\n","\n","\n","```\n","  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.05\n","  cfg.MODEL.RETINANET.TOPK_CANDIDATES_TEST = 1000\n","  cfg.MODEL.RETINANET.NMS_THRESH_TEST = 0.5\n","```\n","\n"," Weights on (dx, dy, dw, dh) for normalizing Retinanet anchor regression targets\n","\n"," ```\n","  cfg.MODEL.RETINANET.BBOX_REG_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n","```\n","\n","Loss parameters\n","\n","![alt text](https://miro.medium.com/max/784/1*FEu_aqp-n1gQ0M-t-OkilQ.png)\n","  ```\n","  cfg.MODEL.RETINANET.FOCAL_LOSS_GAMMA = 2.0\n","  cfg.MODEL.RETINANET.FOCAL_LOSS_ALPHA = 0.25\n","  cfg.MODEL.RETINANET.SMOOTH_L1_LOSS_BETA = 0.3\n","```"]},{"cell_type":"code","metadata":{"id":"Z5MwqbyQH6Ii","colab_type":"code","colab":{}},"source":["def retinanet_setup():\n","  cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n","  cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")  \n","  cfg.MODEL.RETINANET.NUM_CLASSES = 1\n","\n","  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.05\n","  cfg.MODEL.RETINANET.TOPK_CANDIDATES_TEST = 1000\n","  cfg.MODEL.RETINANET.NMS_THRESH_TEST = 0.5\n","\n","\n","  cfg.MODEL.RETINANET.BBOX_REG_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n","\n","  cfg.MODEL.RETINANET.FOCAL_LOSS_GAMMA = 2.0\n","\n","  cfg.MODEL.RETINANET.FOCAL_LOSS_ALPHA = 0.25\n","\n","  cfg.MODEL.RETINANET.SMOOTH_L1_LOSS_BETA = 0.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2j5rClAzVHkX","colab_type":"code","colab":{}},"source":["def faster_rcnn_setup ():\n","  cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n","  cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n","\n","  cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n","  # cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   \n","  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58Ajm0gTZGk_","colab_type":"text"},"source":["General config for all models in detectron2"]},{"cell_type":"code","metadata":{"id":"3Ezp6pEoZmVB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1596727875941,"user_tz":-180,"elapsed":736,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"44046679-ac82-4486-9e5a-9543cd22211e"},"source":["\n","\n","cfg = get_cfg()\n","\n","retinanet_setup()\n","\n","# cfg.MODEL.WEIGHTS =  '/content/gdrive/My Drive/Global Wheat Detection/yonatan_checkpoints/outputs/model_final.pth'\n","cfg.DATASETS.TRAIN = (\"wheat_train\",)\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 4\n","\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025  \n","\n","cfg.SOLVER.GAMMA = 0.05\n","cfg.SOLVER.MAX_ITER = 30000\n","cfg.SOLVER.MOMENTUM = 0.9\n","# Save a checkpoint after every this number of iterations\n","cfg.SOLVER.CHECKPOINT_PERIOD = 10000\n","\n","cfg.TEST.EVAL_PERIOD =1000\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading config /usr/local/lib/python3.6/dist-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kBQwCSsoZh7-","colab_type":"text"},"source":["# Model training\n","\n","In this section we use DefaultTrainer to create a trainer , The trainer will train the model according to config set for him in the section above.\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"LNHoKZYnJa38","colab_type":"code","colab":{}},"source":["cfg.OUTPUT_DIR = '/content/gdrive/My Drive/Global Wheat Detection/yonatan_checkpoints/outputs'\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yiKevVEh5kp","colab_type":"text"},"source":["# Load Model \n","\n","load model with DATASETS.TEST = \"wheat_val\" "]},{"cell_type":"code","metadata":{"trusted":true,"id":"SQ6pzbewJa3_","colab_type":"code","colab":{}},"source":["\n","# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.WEIGHTS =  '/content/gdrive/My Drive/Global Wheat Detection/yonatan_checkpoints/outputs/model_final.pth'\n","# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \n","cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n","cfg.DATASETS.TEST = (\"wheat_val\", )\n","predictor = DefaultPredictor(cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZD_N_7biOMC","colab_type":"text"},"source":["# View predictions on multiple images"]},{"cell_type":"code","metadata":{"trusted":true,"id":"BXG-Sj1XJa4C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZtxT0uDpaWEQakK9SR0rZnIw7h6hjCxE"},"executionInfo":{"status":"ok","timestamp":1596730223298,"user_tz":-180,"elapsed":10255,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"d71f7a34-6310-4194-e231-33db67d31643"},"source":["from detectron2.utils.visualizer import ColorMode\n","dataset_dicts = get_wheat_dicts(val_data)\n","# wheat_metadata = MetadataCatalog.get(\"wheat_val\")\n","for d in random.sample(dataset_dicts, 3):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor(im)\n","    # print(outputs)\n","    v = Visualizer(im[:, :, ::-1],     \n","                   metadata=wheat_metadata, \n","                   scale=0.8, \n","    )\n","    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    plt.figure(figsize=[10, 20])\n","    plt.imshow(v.get_image()[:,:,::-1])\n","    plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"IIEKj8e8Z6mh","colab_type":"text"},"source":["\n","# Understanding Evaluation Metrics\n","\n","\n","---\n","\n","This competition is evaluated on the **mean average precision** at different intersection over union (IoU) thresholds.\n"," \n","To understand mAP, we will explain about precision and recall first.\n","* Recall is the True Positive Rate i.e. Of all the actual positives, how many are True positives predictions. \n","* Precision is the Positive prediction value i.e. Of all the positive predictions, how many are True positives predictions. Read more in evaluation metrics for classification.\n","\n","\n","\n","---\n","## mAP \n","\n","**mAP (mean average precision)** is the average of AP. In some context, we compute the AP for each class and average them. But in some context, they mean the same thing. For example, under the COCO context, there is no difference between AP and mAP."]},{"cell_type":"markdown","metadata":{"id":"imPGZbIxi9Ly","colab_type":"text"},"source":["# Evaluate AR + AP \n","\n","Evaluate AR for object proposals, AP for instance detection/segmentation, AP for keypoint detection outputs using COCO's metrics\n","\n","\n","AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision computes the average precision value for recall value over 0 to 1. \n","\n","![alt text](https://miro.medium.com/max/1925/1*_IkyrFHlqt_xCovk7l0rQQ.png)\n","\n"]},{"cell_type":"code","metadata":{"id":"vh7lV5KzusFN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596730310393,"user_tz":-180,"elapsed":50018,"user":{"displayName":"yonatanbl yonatan","photoUrl":"","userId":"02107808045932675710"}},"outputId":"0b11559e-40fb-47ab-9448-f1f542604773"},"source":["#//AP=46.032 BASE_LR = 0.01 TS=0.5 SOLVER.MOMENTUM=0.9 IMS_PER_BATCH = 8 FOCAL_LOSS_GAMMA = 0.9 FOCAL_LOSS_ALPHA = 0.6 SMOOTH_L1_LOSS_BETA = 0.2//\n","\n","\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n","evaluator = COCOEvaluator(\"wheat_val\", cfg, False, output_dir=\"./output/\")\n","val_loader = build_detection_test_loader(cfg, \"wheat_val\")\n","inference_on_dataset(predictor.model, val_loader, evaluator)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[08/06 16:11:01 d2.evaluation.coco_evaluation]: \u001b[0m'wheat_val' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...\n","\u001b[32m[08/06 16:11:01 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'wheat_val' to COCO format ...)\n","\u001b[32m[08/06 16:11:01 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n","\u001b[32m[08/06 16:11:02 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 338, #annotations: 15116\n","\u001b[32m[08/06 16:11:02 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/wheat_val_coco_format.json' ...\n","\u001b[32m[08/06 16:11:03 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|   wheat    | 15116        |\n","|            |              |\u001b[0m\n","\u001b[32m[08/06 16:11:03 d2.data.common]: \u001b[0mSerializing 338 elements to byte tensors and concatenating them all ...\n","\u001b[32m[08/06 16:11:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.79 MiB\n","\u001b[32m[08/06 16:11:03 d2.data.dataset_mapper]: \u001b[0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[08/06 16:11:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 338 images\n","\u001b[32m[08/06 16:11:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/338. 0.1122 s / img. ETA=0:00:45\n","\u001b[32m[08/06 16:11:10 d2.evaluation.evaluator]: \u001b[0mInference done 50/338. 0.1135 s / img. ETA=0:00:37\n","\u001b[32m[08/06 16:11:15 d2.evaluation.evaluator]: \u001b[0mInference done 89/338. 0.1135 s / img. ETA=0:00:32\n","\u001b[32m[08/06 16:11:20 d2.evaluation.evaluator]: \u001b[0mInference done 126/338. 0.1139 s / img. ETA=0:00:28\n","\u001b[32m[08/06 16:11:25 d2.evaluation.evaluator]: \u001b[0mInference done 165/338. 0.1135 s / img. ETA=0:00:22\n","\u001b[32m[08/06 16:11:31 d2.evaluation.evaluator]: \u001b[0mInference done 204/338. 0.1134 s / img. ETA=0:00:17\n","\u001b[32m[08/06 16:11:36 d2.evaluation.evaluator]: \u001b[0mInference done 242/338. 0.1131 s / img. ETA=0:00:12\n","\u001b[32m[08/06 16:11:41 d2.evaluation.evaluator]: \u001b[0mInference done 274/338. 0.1128 s / img. ETA=0:00:08\n","\u001b[32m[08/06 16:11:46 d2.evaluation.evaluator]: \u001b[0mInference done 315/338. 0.1124 s / img. ETA=0:00:03\n","\u001b[32m[08/06 16:11:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:44.693250 (0.134214 s / img per device, on 1 devices)\n","\u001b[32m[08/06 16:11:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.112357 s / img per device, on 1 devices)\n","\u001b[32m[08/06 16:11:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n","\u001b[32m[08/06 16:11:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n","\u001b[32m[08/06 16:11:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions ...\n","Loading and preparing results...\n","DONE (t=0.02s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","COCOeval_opt.evaluate() finished in 0.29 seconds.\n","Accumulating evaluation results...\n","COCOeval_opt.accumulate() finished in 0.03 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.840\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.481\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.466\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.148\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.527\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.522\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570\n","\u001b[32m[08/06 16:11:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n","|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n","|:------:|:------:|:------:|:-----:|:------:|:------:|\n","| 47.354 | 83.986 | 48.131 | 5.699 | 46.615 | 51.845 |\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('bbox',\n","              {'AP': 47.35426124462239,\n","               'AP50': 83.98597983149305,\n","               'AP75': 48.13063738128962,\n","               'APl': 51.84472146634682,\n","               'APm': 46.614600928733616,\n","               'APs': 5.6988418622082})])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"vqlwIfB5mzmN","colab_type":"text"},"source":["# Submission Section"]},{"cell_type":"code","metadata":{"id":"pNVQITVGWt3F","colab_type":"code","colab":{}},"source":["from pathlib import Path\n","# data_dir = Path('/content/gdrive/My Drive/Global Wheat Detection')\n","# train_img_dir = Path(data_dir / 'train')\n","test_img_dir = Path('/content/gdrive/My Drive/Global Wheat Detection/test')\n","\n","sub_path = Path(data_dir / 'sample_submission.csv')\n","sub_df = pd.read_csv(sub_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zL6ZG8lYWy2j","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","def submit():\n","    for idx, row in tqdm(sub_df.iterrows(), total=len(sub_df)):\n","        img_path = os.path.join(test_img_dir, row.image_id + '.jpg')\n","        \n","        img = cv2.imread(img_path)\n","        outputs = predictor(img)['instances']\n","        boxes = [i.cpu().detach().numpy() for i in outputs.pred_boxes]\n","        scores = outputs.scores.cpu().detach().numpy()\n","        list_str = []\n","        for box, score in zip(boxes, scores):\n","            box[3] -= box[1]\n","            box[2] -= box[0]\n","            box = list(map(int,box))\n","            score = round(score, 4)\n","            list_str.append(score)\n","            list_str.extend(box)\n","        sub_df.loc[idx, 'PredictionString'] = ' '.join(map(str, list_str))\n","        \n","    return sub_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bllVmKRpW0lE","colab_type":"code","colab":{}},"source":["sub_df = submit()\n","sub_df.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mflCCaAOW2Pr","colab_type":"code","colab":{}},"source":["sub_df"],"execution_count":null,"outputs":[]}]}